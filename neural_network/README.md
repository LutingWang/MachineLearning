[toc]

# 使用方法介绍

本文附带源码中包含全部三个模型的源代码。其中 `keras` 目录下是使用 Keras 开源框架搭建的卷积神经网络模型。 `convolutional` 目录下包含手动搭建的 DNN 和 CNN 框架，其中 `fullyconnected.mat` 中包含训练好的浅层神经网络模型， `convolutional.mat` 中包含训练好的卷积神经网络模型。读者可以直接导入并使用网络模型对测试集进行预测验证。

## Keras 开源框架

Keras 是一个 Python 编写的开源神经网络框架，可以作为 Tensorflow 等高阶应用程序接口。使用 Keras 可以在很大程度上降低研发网络模型的时间成本。本文搭建的神经网络模型仅用到了 Keras 的卷积神经网络部分，除此之外 Keras 还支持循环神经网络的设计。使用 Keras 训练的网络可以轻松在测试集上达到 $97\%$ 的正确率。

## DNN

作为卷积神经网络的基础，本文首先使用 Matlab 编写了深度神经网络框架。框架支持的网络结构如下

```mermaid
graph TD
I((input)) --> F[Flatten]
F --> FC[Fully Connected Layers]
FC --sigmoid--> O[Output Layer]
O --> R((Output))
```

尽管框架可以训练具有任意多层的深度网络，但是由于 bp 算法导致的梯度消失等问题，训练效果最好的还是浅层神经网络。保存在 `convolutional/fullyconnected.mat` 文件中的网络模型 `fc` 可以在测试集上达到 $89.5\%$ 的准确率。

## CNN

卷积神经网络广泛应用于计算机视觉领域。在深度神经网络的基础上，只需要新增卷积层和池化层即可完成。一般的卷积网络结构如下

```mermaid
graph TD
I((input)) --> C1[Convolutional Layer]
C1 --reLu--> P1[Max Pooling Layer]
P1 -.-> D[DNN]
```

尽管卷积层的出现很大程度上提高了训练效率，但是训练单个 epoch 的时间也相应延长。保存在 `convolutional/convolutional.mat` 文件中的 `cv` 网络模型在测试集上达到了 $90\%$ 的准确率。

# 方法实现细节

## Keras 开源框架

使用开源框架进行训练相对简单，大体可以分为以下几步

1. 读取数据并进行预处理
2. 搭建网络模型
3. 将数据输入网络模型进行训练
4. 将训练后的网络模型用于新数据的预测

其中每一步都只需用非常简单的代码完成，读者可以查看 `keras` 目录下的 Python 源代码。由于框架具有自动调整训练参数的功能，因此搭建出来的网络模型几乎只有结构参数可以改变。经过几次尝试可以很容易的找到一个比较理想的网络模型，至此训练结束。由于开源框架的准确率较高，而且实现相对简单，因此常用于后文所述模型的比较。

## DNN

本文使用 Matlab 面向对象来搭建深度神经网络框架。框架包括顶层类 `NeuralNetwork` 和 `Layer` 分别用于描述神经网络和网络中各层。考虑到向卷积神经网络的扩展功能，在 DNN 中还引入了 `ComputationalLayer` 作为所有存储了参数的层的超类。在本文中，只有全连接层 `FullyConnected` 和卷积层 `Convolutional` 继承自 `ComputationalLayer` ，其他各层均直接继承自 `Layer` 。

![DNN](./preview/dnn_uml.jpg)

首先从深度神经网络的核心 `FullyConnected` 开始介绍。全连接层的输入和输出都是一维向量，参数是一个二维矩阵。创建时 `FullyConnected` 会自动在 $(-1, 1)$ 之间初始化参数。全连接层类内部还实现了 `Layer` 所定义的两个接口：前向传播和反向传播。前向传播时，当前层根据来自上一层的输入和存储的参数计算出对应的输出，并借由 `NeuralNetwork` 传递给下一层。反向传播时，当前层根据下一层反向传播回来的误差向量以及部分存储信息计算出传向前一层的误差向量，与此同时更新当前层的参数。

除全连接层以外，深度神经网络中 `Flatten` 层和 `Output` 层分别代表输入层和输出层。输入层的功能较为简单，只需要将输入的图片转化为可以被全连接层读取的一维向量即可。用 Matlab 代码实现如下

```matlab
output = reshape(input, obj.inputSize ^ 2 * obj.inputLayers, size(input, 4));
```

输出层的本意是实现 SoftMax 效果，但是由于全连接层本身的激活函数就是 sigmoid ，所以 `Output` 层的前向传播仅仅是将输出向量归一化。用 Matlab 代码实现如下

```matlab
output = input ./ repmat(sum(input), obj.inputSize, 1);
```

与输入层不同，输出层是反向传播的起点。即使输出层本身并没有参数，但仍然需要实现反向传播函数。本文使用的代价函数是 $l_2$ 正则化的交叉熵，所以在输出层的反向传播实现上较为简单

```matlab
delta = obj.input - labels;
```

最后 `NeuralNetwork` 可以根据用户指定的层次顺序构造出一个神经网络，并自动添加输出层。举例来说，可以用以下语句创建一个神经网络

```matlab
nn = NeuralNetwork({ ...
				Flatten(false, 28, 1), ...
				FullyConnected(false, 784, 30), ...
				FullyConnected(true, 30, 10) ...
				});
```

训练时，可以同时指定批次大小和迭代次数

```matlab
nn.train(X_train_bin, Y_train, 100, 50)；
```

为了降低框架的复杂性，步长和正则化惩罚系数被定义在 `Layer` 的静态属性中。每次训练前可以进行更改，使模型分批次以不同参数训练，提高训练的灵活性。

## CNN

相比深度神经网络，卷积神经网络的特点在于卷积层和池化层。相应 UML 如下

![CNN](./preview/cnn_uml.jpg)

由于池化层不包含参数，因此直接继承了 `Layer` 。而 `Convolutional` 需要存储参数并对参数进行更新，所以他是 `ComputationalLayer` 的子类。尽管这两个类是卷积神经网络所特有的，但是他们的兄弟类已经在深度神经网络一节中被详细介绍了，因此此处不会详细说明其技术细节。

不过值得一提的是网络的计算效率问题。尽管我们的输入都是二维图片，但是卷积层中可能包含多个卷积核，因此对于一般的情况还是需要假设卷积层的输入是一个多通道图片。与此同时，我们的神经网络需要支持 minibatch 机制，因此卷积层之间传递的是一个四维矩阵 $s \times s \times c \times m$ 。

再来看卷积层的参数。一个卷积核需要同时处理输入的所有通道，因此至少是三维的。而每个卷积核又只能对应一个输出通道，所以卷积层的参数也是一个四维矩阵。由于卷积操作的特殊性，这里我们无法调用 Matlab 内置的 `convn` 函数，只能对全部样本进行遍历。这意味着无论 minibatch 的大小设置为多少，一个 epoch 都至少要执行 $60000$ 次循环。对于池化层，循环内部的操作更加复杂。

正因如此，本框架放弃了表现效果更好的 `MaxPooling` ，而是实现了较为快速的 `AveragePooling` 。训练过程中也尽量减少卷积和池化的操作次数。这在一定程度上牺牲了准确率，以换得效率上的提升。

## 数据预处理与参数初始化

输入图片是 $28 \times 28$ 的灰度图像，图像中每个像素都用 $0 \sim 255$ 的光度值来表示。但是通过可视化部分样本可以发现，光度值的具体大小对于图像显示没有很大区别。同时，在同一片亮色区域可能有光度波动，这样的噪声可能会影响模型的学习效果。因此，训练前首先将亮色区域设置为 $1$ ，其他区域设置为 $0$ 。使用 Matlab 实现如下

```matlab
X_train_bin = X_train > 20;
```

这样原始图像就被压缩成一个 01 矩阵。为了使输入输出以及参数在数量级上大致相当，取参数的初始值范围在 $(-1, 1)$ 之间，以加速训练过程。

# 模型结构

本文共涉及三个神经网络模型，下面首先列出其创建语句

1. Keras 框架

```python
model = keras.models.Sequential()
model.add(keras.layers.Conv2D(32, (5, 5), input_shape = (28, 28, 1), activation = 'relu'))
model.add(keras.layers.MaxPooling2D(pool_size = (2, 2)))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(units = 128, activation = 'relu'))
model.add(keras.layers.Dense(units = 10, activation = 'softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
```

2. DNN

```matlab
fc = NeuralNetwork({ ...
				Flatten(false, 28, 1), ...
				FullyConnected(false, 784, 30), ...
				FullyConnected(true, 30, 10) ...
				});
```

3. CNN

```matlab
cv = NeuralNetwork({ ...
				Convolutional(false, 3, 1, 5), ...
				AveragePooling(false, 2), ...
				Flatten(false, 13, 5), ...
				FullyConnected(false, 13 ^ 2 * 5, 30), ...
				FullyConnected(true, 30, 10) ...
				})
```

代码的含义都比较明显。其中最复杂的模型是使用 Keras 框架时使用的，因为开源框架可以自动处理调参过程，同时运行效率较高。在这个模型中，使用了一个 $32$ 核的卷积层和一个具有 $128$ 个神经元的全连接层。相比之下，自己实现的卷积神经网络就只使用了一个 $5$ 核的卷积层和一个具有 $30$ 个神经元的全连接层。正如前面所说，这是一个对于运行速度的折衷。

本文中最简单的模型是一个单隐层的神经网络。原本这个网络的效果应该是低于卷积神经网络的，因为卷积神经网络中包含卷积层和池化层。但是由于这个模型不包含卷积操作，因此可以快速迭代训练。在训练近 $1000$ 个 epoch 后，此模型的准确率超过了卷积神经网络。尽管卷积神经网络也被训练了差不多的时间，但仅仅完成了 $10$ 余个 epoch 。

# 调参过程

Keras 框架自带调参功能，此处主要说明后两个模型的调参过程。从上面的讨论可以看出，模型一共包含五个参数，分别是

| 参数       | 功能                           |
| ---------- | ------------------------------ |
| $\epsilon$ | 限制网络中每个参数初始值的大小 |
| $\alpha$   | 反向传播梯度下降学习率         |
| $\lambda$  | 正则化惩罚项系数               |
| batchSize  | 训练批次大小                   |
| epochs     | 单次训练样本集迭代次数         |

基于本文的实现，模型训练可以分段进行，因此 batchSize 和 epochs 这两个参数几乎与模型训练效果无关（即使一次训练过程中 batchSize 大小导致了训练效果不理想，后续还可以通过增加训练次数来弥补）。而 $\epsilon$ 只用于模型初始化，且其设置在前文中已经讨论过，因此也不在调参范围内。这样来看，实际对模型构成影响的只有 $\alpha$ 和 $\lambda$ 两项。

现在假设模型的实现是正确的，也就是说不存在结构上的致命错误。我的调参过程如下

1. 将 $\alpha$ 和 $\lambda$ 调到初始值
2. 对样本进行少量训练
3. 使用模型对训练样本进行预测，记录 confidence 和 accuracy
4. 如果 confidence 或 accuracy 有少许提升，则根据提升幅度增大训练量，并重复第 3 步；否则对 $\alpha$ 或 $\lambda$ 进行修改，并重复第二步

对于上述过程有几点需要说明。首先样本的训练量是依赖于模型的。一般来说卷积神经网络每次最多训练 $5$ 个 epoch 。过多的训练量不仅会消耗很长时间，而且会放大梯度爆炸的影响，使模型参数变化范围超过其调节范围。这类似于超过模型的弹性限度，此时模型就无法再用于后续训练了。而单纯的深度网络则可以一次训练 $20 \sim 50$ 个 epoch 。具体的原因还不能确定，但笔者猜测这与卷积神经网络的参数共享有关，因为共享的参数一旦发生变化将影响整个输入图片。

其次 $\alpha$ 和 $\lambda$ 的初始值依赖于训练量。如前所述，过大的 $\alpha$ 和 $\lambda$ 同样会导致梯度爆炸。因此对于卷积神经网络一般设置初始值为 $0.05$ 和 $0.005$ ，而对于普通深度网络则需要减小一个数量级。这个数值是在进行过多次调整后由经验确定的，对于不同的模型可能也有所出入。但在缺乏先验知识的情况下，这组数值可能是一个很好的开头。

另外，训练样本时记录的 confidence 和 accuracy 分别表示预测的置信度和准确率。其中置信度完全由模型生成，即将全部测试样本输入模型后得到的输出。而准确率则需要根据预测结果和实际标签比对生成。一般来说，准确率可以比较好的反应模型预测效果有没有提升。但是一旦发生过拟合，单纯追求准确率可能会陷入恶性循环。因此比较好的做法是检查几个样本的置信度变化，这些样本一般是前次训练效果不理想的点。如果某次训练使这些样本的置信度有所提升，那么可以放心的使用当前参数进行大量样本训练。否则需要检查整体准确率是否提升，因为又看抽取的样本恰好没有明显变化。如果准确率有上升，则需要视情况继续训练或更改参数。

最后，修改参数值的经验是先减小正则化系数，如果无效再减小步长，最后增加训练量。实际调参过程中，很少会遇到增大 $\alpha$ 或 $\lambda$ 的情况（只有在准确率持续小幅增长时才会考虑）。比较常见的情况是准确率增幅越来越慢，且渐进趋势不令人满意。这种情况一般要先考虑减小正则化系数。因为增幅减慢仍然处于增长状态，所以一般不是因为步长过大。而正则化系数过大则确实会降低模型预测能力的上界。为了避免由正则化引入的精度损失，调参首选应该是减小正则化系数。

如果训练到网络接近饱和时，之前减小的步长可能使训练速度进一步变慢，因此这时可以选择逐渐增大步长。如果希望预防过拟合也可以同步增大惩罚项系数。以卷积神经网络的调参过程为例，参数大小的变化如下

| 编号 | 学习率 $\alpha$ | 惩罚项系数 $\lambda$ |
| :--: | --------------- | -------------------- |
|  1   | 0.05            | 0.005                |
|  2   | 0.05            | 0.001                |
|  3   | 0.05            | 0.0005               |
|  4   | 0.01            | 0.0005               |
|  5   | 0.01            | 0.0003               |
|  6   | 0.005           | 0.0003               |
|  7   | 0.008           | 0.0003               |
|  8   | 0.01            | 0.0005               |

表中的每次调整可能对应多次训练，每次训练也可能包含不同的迭代次数。因此表中数据仅作为说明。

# 问题及解决方法

本文所述问题可以大致分为三个层次

1. 框架级：神经网络框架的代码实现有误
2. 模型级：使用框架构造出的模型有误
3. 训练级：训练模型时的调参方法不当

本节将按次序说明遇到的问题以及相应的查错和解决方法。

## 计算公式输入错误

第一次遇到这个错误是在深度学习框架刚刚编写完成时。为了检验框架是否正常工作，笔者随机生成了一组测试样例。样例输入为一个二维点坐标，标准输出为该点所属象限。理论上说这是一个非常简单的分类任务，不需要太多的训练次数。但是当时的模型在训练 $5$ 个 epoch 后返回了全 $1$ 的结果，意味着网络完全没有进行学习。

查看权重矩阵发现矩阵中每个元素都相等，但随机初始化权重的函数已经通过了单元测试，因此将错误定位到前向传播和反向传播。在输出层设置断点发现，前一层的输出非常随机，而且有些值会超过 $1$ 。但是输出层的前层为全连接层，其输出应在 $(0, 1)$ 之间。再检查全连接层的前向传播函数时，发现没有对神经元状态进行激活。

在修改了前向传播公式后，重新进行测试发现，网络的输出仅仅是复现上一个训练样本的标签。最初的想法是学习率过大导致单个样本造成的权重波动，但是减小学习率后还是没有改观。再次查看权重矩阵发现，矩阵中元素的差距不大，但是每次更新都会使其中某个元素超过同一行中的其他值。这个现象原本是正常的，但问题在于权重的增长不能被积累下来。而网络中可能导致权重减小的只有 $l_2$ 正则化，因此将惩罚项系数调整为 $0$ 。继续训练后，权重出现了爆炸性增长，进而怀疑反向传播过程出现了问题。仔细排查后，发现反向传播公式录入有误。修改正确后，模型成功在测试样例上达到了 $80\%$ 的正确率。

## 偏移项导致的矩阵维数不匹配

在反向传播的公式推导过程中，一般会忽略偏移项产生的影响。但是在实际实现中，如果忽略则会导致矩阵加法无法完成。 

# 评测分数和排名

